Problem 4:

**** Import orders table from mysql as text file to the destination /user/cloudera/problem5/text. 
	 Fields should be terminated by a tab character (“\t”) character and lines should be terminated by new line character (“\n”).
	 
	 
**** Import orders table from mysql  into hdfs to the destination /user/cloudera/problem5/avro. File should be stored as avro file.	 
	 
	 
**** Import orders table from mysql  into hdfs  to folders /user/cloudera/problem5/parquet. File should be stored as parquet file.	 
	 
	 
**** Transform/Convert data-files at /user/cloudera/problem5/avro and store the converted file at the following locations and file formats
	save the data to hdfs using snappy compression as parquet file at /user/cloudera/problem5/parquet-snappy-compress
	save the data to hdfs using gzip compression as text file at /user/cloudera/problem5/text-gzip-compress
	save the data to hdfs using no compression as sequence file at /user/cloudera/problem5/sequence
	save the data to hdfs using snappy compression as text file at /user/cloudera/problem5/text-snappy-compress	 
	 
	 
****Transform/Convert data-files at /user/cloudera/problem5/parquet-snappy-compress and store the converted file at the following locations and file formats
	save the data to hdfs using no compression as parquet file at /user/cloudera/problem5/parquet-no-compress
	save the data to hdfs using snappy compression as avro file at /user/cloudera/problem5/avro-snappy  
	

**** Transform/Convert data-files at /user/cloudera/problem5/avro-snappy and store the converted file at the following locations and file formats
	save the data to hdfs using no compression as json file at /user/cloudera/problem5/json-no-compress
	save the data to hdfs using gzip compression as json file at /user/cloudera/problem5/json-gzip


**** Transform/Convert data-files at  /user/cloudera/problem5/json-gzip and store the converted file at the following locations and file formats
	save the data to as comma separated text using gzip compression at   /user/cloudera/problem5/csv-gzip
	


**** Using spark access data at /user/cloudera/problem5/sequence and 
	stored it back to hdfs using no compression as ORC file to HDFS to destination /user/cloudera/problem5/orc
	
	
	
	
	
sqoop --import --connect jdbc:mysql://nn01.itversity.com/retail_db  --username retail_dba --password itversity \
--table orders 
--fields-terminated-by "\t" 
--lines-terminated-by "\n"
--target-dir "/user/cloudera/problem5/text"


sqoop --import --connect jdbc:mysql://nn01.itversity.com/retail_db  --username retail_dba --password itversity \
--table orders
--as-avrodatafile
--target-dir "/user/cloudera/problem5/avro"


sqoop --import --connect jdbc:mysql://nn01.itversity.com/retail_db  --username retail_dba --password itversity \
--table orders 
--as-parquetfile
--target-dir "/user/cloudera/problem5/parquet"

sqlContext.sql("spark.sql.parquet.compress.codec","snappy")
val df = sqlContext.read.avro("/user/cloudera/problem5/avro")

	df.write.mode(SaveMode.Append).format("parquet").save("/user/cloudera/problem5/parquet-snappy-compress")
	df.repartition(1).write.parquet("/user/cloudera/problem5/parquet-snappy-compress");
	
-- save as TextFile as well use gzip Compression 

	df.map( rec => rec.mkString(",")).saveAsTextFile("/user/cloudera/problem5/text-gzip-compress",classOf[org.apache.hadoop.io.compress.GzipCodec]);
	
-- save as Sequence File 

	df.map( rec => (rec.split(",")(0).toString , rec.mkString(",") ).saveAsSequenceFile("/user/cloudera/problem5/sequence")

-- text File SnappyCompress 

	df.map( rec => rec.split(",").mkString("|")).saveAsTextFile("/user/cloudera/problem5/text-snappy-compress" ,classOf[org.apache.hadoop.io.compress.snappyCodec])

	
-- To uncompress the Data 

	sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
	val df = sqlContext.read.parquet("/user/cloudera/problem5/parquet-snappy-compress")
	df.write.parquet("/user/cloudera/problem5/parquet-no-compress")
	
	
	sqlContext.setConf("spark.sql.avro.compression.codec","Snappy")
	val df = sqlContext.read.avro("/user/cloudera/problem5/parquet-snappy-compress")
	df.write.avro("/user/cloudera/problem5/avro-snappy")
	
-- avro snappy 

	var avroData = sqlContext.read.avro("/user/cloudera/problem5/avro-snappy");
	avroData.toJSON.saveAsTextFile("/user/cloudera/problem5/json-no-compress");
	avroData.toJSON.saveAsTextFile("/user/cloudera/problem5/json-gzip", classOf[org.apache.hadoop.io.GzipCodec]);

-- Json gZip to csv Gzip 
	val df = sqlContext.read.json("/user/cloudera/problem5/json-gzip")
	df.map( rec => rec.mkstring(",")).saveAsTextFile("/user/cloudera/problem5/csv-gzip",classOf[org.apache.hadoop.io.compress.gZip])   //map(x=>x(0)+","+x(1)+","+x(2)+","+x(3)). 
	
	
	
Step 8 


// Step 8: 
//To read the sequence file you need to understand the sequence getter for the key and value class to //be used while loading the sequence file as a spark RDD.
//In a new terminal Get the Sequence file to local file system
hadoop fs -get /user/cloudera/problem5/sequence/part-00000
//read the first 300 characters to understand the two classes to be used. 
cut -c-300 part-00000

//In spark shell do below
var seqData = sc.
  sequenceFile("/user/cloudera/problem5/sequence/", classOf[org.apache.hadoop.io.Text], classOf[org.apache.hadoop.io.Text]);
seqData.
  map(x => {
    var d = x._2.toString.split("\t"); (d(0),d(1),d(2),d(3))
  }).
  toDF().
  write.
  orc("/user/cloudera/problem5/orc");
  
  
	