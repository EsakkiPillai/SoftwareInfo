instead of creating multiple channels it will create a queue and push the data those are subscribe will receive the data 
publish subscribe process 


Zookeeper running on nn01 nn02 rm01 portno 2181 

# Run every time you login or update .bash_profile
export KAFKA_HOME=/usr/hdp/2.5.0.0-1245/kafka
PATH=$PATH:$KAFKA_HOME/bin

# Create topics
kafka-topics.sh --create \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com \
  --replication-factor 1 \
  --partitions 1 \
  --topic kafkadg

# List all topics
kafka-topics.sh --list \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com

# List one topic
kafka-topics.sh --list \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com \
  --topic mytopicdg

kafka-topics.sh --delete \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com \
  --topic testdg

# Command to produce messages, start typing after running this kakfa-console-producer command
# The messages will be stored in topic kafkadg on the host where brokers are running
kafka-console-producer.sh \
  --broker-list nn01.itversity.com:6667,nn02.itversity.com:6667,rm01.itversity.com:6667 \
  --topic kafkadg

# Open another shell and then run kafka-console-consumer command to see streaming messages
kafka-console-consumer.sh \
  --bootstrap-server nn01.itversity.com:6667,nn02.itversity.com:6667,rm01.itversity.com:6667 \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com \
  --topic kafkaesak \
  --from-beginning
  
  
  
  In Flume we can use Kafka As a 	Source => it will consume the msg from the topic 
									channel => it publish the message to topic it will consume the msg from sinks 
	
	Mostly it will be used as a Sink 
	
	
  
  