Problem 7:

**** This step comprises of three substeps. Please perform tasks under each subset completely
	 using sqoop pull data from MYSQL orders table into /user/cloudera/problem7/prework as AVRO data file using only one mapper
	 Pull the file from \user\cloudera\problem7\prework into a local folder named flume-avro
	 create a flume agent configuration such that it has an avro source at localhost and port number 11112, 
	 a jdbc channel and an hdfs file sink at /user/cloudera/problem7/sink
	 Use the following command to run an avro client flume-ng avro-client -H localhost -p 11112 -F <<Provide your avro file path here>>
	 
	

**** The CDH comes prepackaged with a log generating job. start_logs, stop_logs and tail_logs. Using these as an aid and provide a solution to below problem. The generated logs can be found at path /opt/gen_logs/logs/access.log
	run start_logs
	write a flume configuration such that the logs generated by start_logs are dumped into HDFS at location /user/cloudera/problem7/step2. 
	The channel should be non-durable and hence fastest in nature. 
	The channel should be able to hold a maximum of 1000 messages and should commit after every 200 messages.
	Run the agent.
	confirm if logs are getting dumped to hdfs.
	run stop_logs.



	
sqoop import --connect jdbc://mysql:nn01.itversity.com:3306/retail_db --username retail_dba --password itversity
--table orders 
-target-dir "/user/cloudera/problem7/prework"
--as-avrodatafile
--m 1

Pull the file from \user\cloudera\problem7\prework into a local folder named flume-avro
	hdfs -get \user\cloudera\problem7\prework  /user/esakkipillai/problem7/flume-avro
	
create a flume agent configuration such that it has an avro source at localhost and port number 11112,
		
### The Following Configuration is used for the Flume spark Hdfs 
# Describe the Agent Here
ag1.sources = avrosource 
ag1.sinks = hdfssink 
ag1.channels = jdbcchannel 

#Desccribe the Source 
ag1.sources.avrosource.type = avro
ag1.sources.avrosource.bind = localhost
ag1.sources.avrosource.port = 111112

#Describe the Hdfssink
ag1.sinks.hdfssink.type= hdfs
ag1.sinks.hdfssink.hdfs.path= /user/cloudera/problem7/sink
ag1.sinks.hdfssink.hdfs.filePrefix = execsource-%d-%m-%y
ag1.sinks.hdfssink.hdfs.fileSuffix=.avro
ag1.sinks.hdfssink.hdfs.useLocalTimeStamp=true
ag1.sinks.hdfssink.hdfs.rollInterval = 120

#Describe the File Channel 
ag1.channels.hdfschannel.type = jdbc


# Bind the Components
ag1.sources.avrosource.channels=jdbcchannel 
ag1.sinks.hdfssink.channel = jdbcchannel


To Run the Flume Agent 
flume-ng agent --name ag1 --conf <conf Directory fullpath > --conf-file <CONFFILE FULL PATH >



flume-ng avro-client -H localhost -p 11112 -F <<Provide your avro file path here>>




### The Following Configuration is used for the Flume spark Hdfs 
# Describe the Agent Here
ag2.sources = logFileSource 
ag2.sinks = hdfssink 
ag2.channels = memChannel 

#Desccribe the Source 
ag2.sources.logFileSource.type = exec 
ag2.sources.logFileSource.command  = tail -f /opt/gen_logs/logs/access.log

#Describe the Hdfssink
ag2.sinks.hdfssink.type= hdfs
ag2.sinks.hdfssink.hdfs.path= /user/cloudera/problem7/sink
ag2.sinks.hdfssink.hdfs.filePrefix = execsource-%d-%m-%y
a1.sinks.k1.hdfs.fileSuffix = .log
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.fileType = DataStream
ag2.sinks.hdfssink.hdfs.useLocalTimeStamp=true
ag2.sinks.hdfssink.hdfs.rollInterval = 120

#Describe the File Channel 
ag2.channels.memChannel.type = memory
ag2.channels.c1.capacity = 1000
ag2.channels.c1.transactionCapacity = 200


# Bind the Components
ag2.sources.logFileSource.channels=memChannel 
ag2.sinks.hdfssink.channel = memChannel


