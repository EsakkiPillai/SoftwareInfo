val src = sc.textFile()
val lines = src.flatMap( line => line.split(" "))
val word = lines.map( x => (x,1))
val counts = word.reducebykey( x,y =>  x+y)
counts.saveAstextFile("path to the File")

we can also use the below Method as Well 
val src = sc.textFile()
val lines = src.flatMap( line => line.split(" "))
val counts = lines.countByValues()  """ it will return the Map Collection """


First Sample Application 


object WordCount{

def main (args:Array[String]){

// Create A Spark Session 

val conf = new sparkConf().setAppName("First Demo Appln")
val sc = new sparkContext(conf)

the Above Two Lines will be Equivalent to opening a Spark-scala Terminal => which already have sc value  the above configuration is for spark 1.6 for 2.0 we have to call the SparkSession 

val src = sc.textFile()
val lines = src.flatMap( line => line.split(" "))
val word = lines.map( x => (x,1)) 
val counts = word.reducebykey( x,y =>  x+y)             word is of RDD rdd dont have reducebyKey Method so scala will convert this rdd in to the form which has the method in our case rdd will be transfered in to PAIR RDDFunctions 
counts.saveAstextFile("path to the File")


}
}


Tro sumbit Appln 

spark-submit --class "jarname" "InputFilePath" "OutputFilePath"
===================================================================================================================================================================================================================

HOW AGGREGATES WORKS :- 

First of all Thanks to Diego's reply which helped me connect the dots in understanding aggregate() function..

Let me confess that I couldn't sleep last night properly because I couldn't get how aggregate() works internally, I'll get good sleep tonight definitely :-)

Let's start understanding it

val result = List(1,2,3,4,5,6,7,8,9,10).par.aggregate((0, 0))
         (
          (x, y) => (x._1 + y, x._2 + 1), 
          (x,y) =>(x._1 + y._1, x._2 + y._2)
         )
result: (Int, Int) = (55,10)

aggregate function has 3 parts :

initial value of accumulators : tuple(0,0) here
seqop : It works like foldLeft with initial value of 0
combop : It combines the result generated through parallelization (this part was difficult for me to understand)
Let's understand all 3 parts independently :

part-1 : Initial tuple (0,0)

Aggregate() starts with initial value of accumulators x which is (0,0) here. First tuple x._1 which is initially 0 is used to compute the sum, Second tuple x._2 is used to compute total number of elements in the list.

part-2 : (x, y) => (x._1 + y, x._2 + 1)

If you know how foldLeft works in scala then it should be easy to understand this part. Above function works just like foldLeft on our List(1,2,3,4...10).

Iteration#      (x._1 + y, x._2 + 1)
     1           (0+1, 0+1)
     2           (1+2, 1+1)
     3           (3+3, 2+1)
     4           (6+4, 3+1)
     .             ....
     .             ....
     10          (45+10, 9+1)
thus after all 10 iteration you'll get the result (55,10). If you understand this part the rest is very easy but for me it was the most difficult part in understanding if all the required computation are finished then what is the use of second part i.e. compop - stay tuned :-)

part 3 : (x,y) =>(x._1 + y._1, x._2 + y._2)

Well this 3rd part is combOp which combines the result generated by different threads during parallelization, remember we used 'par' in our code to enable parallel computation of list :

List(1,2,3,4,5,6,7,8,9,10).par.aggregate(....)

Apache spark is effectively using aggregate function to do parallel computation of RDD.

Let's assume that our List(1,2,3,4,5,6,7,8,9,10) is being computed by 3 threads in parallel. Here each thread is working on partial list and then our aggregate() combOp will combine the result of each thread's computation using the below code :

(x,y) =>(x._1 + y._1, x._2 + y._2)
Original list : List(1,2,3,4,5,6,7,8,9,10)

Thread1 start computing on partial list say (1,2,3,4), Thread2 computes (5,6,7,8) and Thread3 computes partial list say (9,10)

At the end of computation, Thread-1 result will be (10,4), Thread-2 result will be (26,4) and Thread-3 result will be (19,2).

At the end of parallel computation, we'll have ((10,4),(26,4),(19,2))

Iteration#      (x._1 + y, x._2 + 1)
     1           (0+10, 0+4)
     2           (10+26, 4+4)
     3           (36+19, 8+2)
which is (55,10).

Finally let me re-iterate that seqOp job is to compute the sum of all the elements of list and total number of list whereas combine function's job is to combine different partial result generated during parallelization.

I hope above explanation help you understand the aggregate().



def computeAvg(input: RDD[Int]) = {
    input.aggregate((0, 0))((x, y) => (x._1 + y, x._2 + 1),
      (x,y) => (x._1 + y._1, x._2 + y._2))
  }
  
  
  val avgFoldLeft  = (src.foldLeft(0)((r,c)=> r+c )) / (src.foldLeft(0)((sum,_) => sum+1))
====================================================================================================================================================================================================================================




val master = args.length match {
case x :Int if x> 0 => arg[0] 
case _ => "local"
}

if the length of the Argument Matches is Greater than 0 then it will take the atgs[0] if not it will take the argument as "Local"


===========================================================================================================================================================================================






def parsedata(line:String)  = {


val rdd = src.map(line => line.split(","))
val custid= rdd.map( x => x_.1)
val amount = rdd.map( x => x._2)
(custid ,amount)

}
val src = sc.textFile()
val data = src.map(parsedata)
val  group = data.mapValues( x => (x ,1) )    (44 ,( 
val count = group.reducebyKey((x,y) => (x._1+y._1 , x._2+y._2))
val result = count.map ( x =>  val custid = x._1 val amountSpend = x._2.tofloat()  ( amountspend , custid).sortbyKey())
result.collect()




-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

ReduceByKey 

it will take 2 values (x,y) => dataset (k1,v1) (k2,v2) (k1,v2) (k2,v10)

src.reduceByKey((x,y) => (x+y) )

if the dataaset is like (k1,(v1,v2)) (k2,(v2,v4))

src.reduceByKey((x,y) => (x._1 + y._1 , x._2 + y._2 )

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
We Should Avoid GroupByKey whenever is Possible , it will shufflle and aggregate it so it will involve in huge network performance in the Cluster
groupByKey()

groupBykey always  groupbased on the Key 


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Fold 

it requires a initial Value 


Step 1:- 
Update build.sbt with below log4j dependency
libraryDependencies += "log4j" % "log4j" % "1.2.14"

step 2:- 
create Log4j.properties file under src/main/resources and update as follows:

# Define the root logger with appender file
log = /tmp/log4j
log4j.rootLogger = DEBUG, FILE

# Define the file appender
log4j.appender.FILE=org.apache.log4j.FileAppender
log4j.appender.FILE.File=${log}/log.out

# Define the layout for file appender
log4j.appender.FILE.layout=org.apache.logs4j.PatternLayout
log4j.appender.FILE.layout.conversionPattern=%m%n
SBT run, can be seen in /tmp/log4j/log.out log

Step 3:- Can be used in code as follows:

import org.apache.log4j.Logger

object HelloWorld {
  val logger = Logger.getLogger(this.getClass.getName)
  def main(args: Array[String]): Unit = {
    logger.info("Logger : Welcome to log4j")
  }
}

=====================================================================================================================================================================
Merging al the Small Files into a Single Large File 

Here is one more alternate, this is still the legacy approach pointed out by @Andrew in his comments but with extra steps of making your input folder as a buffer to receive small files pushing them to a tmp directory in a timely fashion and merging them and pushing the result back to input.

step 1 : create a tmp directory

hadoop fs -mkdir tmp
step 2 : move all the small files to the tmp directory at a point of time

hadoop fs -mv input/*.txt tmp
step 3 -merge the small files with the help of hadoop-streaming jar

hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \
                   -Dmapred.reduce.tasks=1 \
                   -input "/user/abc/input" \
                   -output "/user/abc/output" \
                   -mapper cat \
                   -reducer cat


If you want compression add
-Dmapred.output.compress=true \ -Dmapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec
				   
step 4- move the output to the input folder

hadoop fs -mv output/part-00000 input/large_file.txt
step 5 - remove output

 hadoop fs -rm -R output/
step 6 - remove all the files from tmp

hadoop fs -rm tmp/*.txt
Create a shell script from step 2 till step 6 and schedule it to run at regular intervals to merge the smaller files at regular intervals (may be for every minute based on your need)

Steps to schedule a cron job for merging small files

step 1: create a shell script /home/abc/mergejob.sh with the help of above steps (2 to 6)

important note: you need to specify the absolute path of hadoop in the script to be understood by cron

#!/bin/bash
/home/abc/hadoop-2.6.0/bin/hadoop fs -mv input/*.txt tmp
wait
/home/abc/hadoop-2.6.0/bin/hadoop jar /home/abc/hadoop-2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar \
                   -Dmapred.reduce.tasks=1 \
                   -input "/user/abc/input" \
                   -output "/user/abc/output" \
                   -mapper cat \
                   -reducer cat
wait
/home/abc/hadoop-2.6.0/bin/hadoop fs -mv output/part-00000 input/large_file.txt
wait
/home/abc/hadoop-2.6.0/bin/hadoop fs -rm -R output/
wait
/home/abc/hadoop-2.6.0/bin/hadoop fs -rm tmp/*.txt
step 2: schedule the script using cron to run every minute using cron expression

a) edit crontab by choosing an editor

>crontab -e
b) add the following line at the end and exit from the editor

* * * * * /bin/bash /home/abc/mergejob.sh > /dev/null 2>&1
The merge job will be scheduled to run for every minute.

Hope this was helpful.

IN SPARK 

If you are writing spark jobs and want to get a merged file to avoid multiple RDD creations and performance bottlenecks use this piece of code before transforming your RDD

sc.textFile("hdfs://...../part*).coalesce(1).saveAsTextFile("hdfs://...../filename)

This will merge all part files into one and save it again into hdfs location

--------------------------------------------------------------------------------------------------------------------------------------------------------
HAR FILE
Hadoop Archives or HAR is an archiving facility that packs files in to HDFS blocks efficiently and hence HAR can be used to tackle the small files problem in Hadoop. HAR is created from a collection of files and the archiving tool (a simple command) will run a MapReduce job to process the input files in parallel and create an archive 

**HAR command
-----------
hadoop archive -archiveName myhar.har /input/location /output/location

Once a .har file is created, you can do a listing on the .har file and you will see it is made up of index files and part files. Part files are nothing but the original files concatenated together in to a big file. Index files are look up files which is used to look up the individual small files inside the big part files.

hadoop fs -ls /output/location/myhar.har

/output/location/myhar.har/_index
/output/location/myhar.har/_masterindex
/output/location/myhar.har/part-0

***Limitations of HAR files
----------------------------
Once an archive file is created, you can not update the file to add or remove files. In other words, har files are immutable.
Archive file will have a copy of all the original files so once a .har is created it will take as much space as the original files. Don’t mistake .har files for compressed files.
When a .har file is given as an input to MapReduce job, the small files inside the .har file will be processed individually by separate mappers which is inefficient.


===========================================================================================================================================================================================================
Loading From External Sources 
------------------------------------------------------
JDBC 
------
create table employee(emp_id INT NOT NULL AUTO_INCREMENT,emp_name VARCHAR(100),emp_sal INT,PRIMARY KEY(emp_id))

val connection ="jdbc:mysql://nn01.itversity.com/<DBNAME>"

// Mysql Connection Properties 
val mysql_props = new java.util.Properties
mysql_props.setProperty("user","root")
mysql_props.setProperty("password","esak@123")

//load the data from jdbc data source 
val employee = sqlContext.read.jdbc(connection,"employee",mysql_props)

// cehck the dataframe 

employee.show()

// register this dataframe as temp table 

employee.registerTempTable("employeeDF")

sqlContext.sql("select 8 from employeeDF")

--------------------------------------------------------------------

Loading from the CSV File 
-------------------------

val data = sqlContext.load("com.databricks.spark.csv",Map("path"->args(1),"header"->true))
data.printSchema()

saving the Csv File 
---------------------

val df = sqlContext.load("org.apache.spark.sql.json",Map("path"->args(1))
df.save("com.databricks.spark.csv",SaveMode.ErrorIfExists,Map("path"->args(2),"header"->true))

================================================================================================================================================================================================
Save Modes
Save operations can optionally take a SaveMode, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing a Overwrite, the data will be deleted before writing out the new data.

Scala/Java							Any Language			Meaning
SaveMode.ErrorIfExists (default)	"error" (default)	When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.
SaveMode.Append						"append"			When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.
SaveMode.Overwrite					"overwrite"			Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.
SaveMode.Ignore						"ignore"			Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.




df.select("name", "age", "sex").write.mode(SaveMode.Append).format("parquet").save("/user/esakkipillai/person")

df.write.parquet("/user/esakkipillai/person_2")
Second statement fails to execute since it takes save mode as an "error" and raises below error.
org.apache.spark.sql.AnalysisException : path hdfs://cloudera1:8020//user/esakkipillai/person already exists


**When Working with the HIveContext, we can store the dataframe as persistent table using saveAsTable Command.
**Unlike the registerTempTable command it will be materialize the content of the dataframe and create a pointer to the data in the hive metastore.
**Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore
**By default saveAsTable will create a “managed table”
** data will be available location will be controlled by the metastore 
** When the Managed table is dropped data will be dropped as well 

**sql supports both reading and writiing parquet files 
** parquet file is a columnar format it automatically preserves the schema of the data 
**when writing the data to the parquet files, all the columns are automatically converted to null for compatibility reasons 


NaN Semantics
There is specially handling for not-a-number (NaN) when dealing with float or double types that does not exactly match standard floating point semantics. Specifically:

NaN = NaN returns true.
In aggregations all NaN values are grouped together.
NaN is treated as a normal value in join keys.
NaN values go last when in ascending order, larger than any other numeric value.


**** DataFrames 
**In Spark, a DataFrame is a distributed collection of data organized into named columns. 
**It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. 
**DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.

**** DataFrames are evaluted lazily 
**** which means no computation take place until an action is  performed 
**** action is any method that not produces dataframe, Such as data on the console converting data into scala arrays storing in a file 

rdd.toDF()  			// this implicit conversion creates a DataFrame with column name _1 and _2
rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"

Column Level Operations 

def /(other: Any): Column   Division this expression by another expression. 
// Scala: The following divides a person's height by their weight.
			people.select( people("height") / people("weight") )


def ===(other: Any): Column 	Equality test.	
// Scala:  checking the columns 
df.filter( df("colA") === df("colB") )


// Scala: The following selects people older than 21.
people.select( people("age") > 21 )
// Renames colA to colB in select output.
df.select($"colA".alias("colB"))
// Renames colA to colB in select output.
df.select($"colA".as('colB))

// Scala: sort a DataFrame by age column in ascending order.
df.sort(df("age").asc)

// Casts colA to IntegerType.
import org.apache.spark.sql.types.IntegerType
df.select(df("colA").cast(IntegerType))

// Scala
df.sort(df("age").desc)

def isNaN: Column
True if the current expression is NaN.

def isNotNull: Column
True if the current expression is NOT null.

def isNull: Column
True if the current expression is  null.

def substr(startPos: Int, len: Int): Column
An expression that returns a substring.

// Scala: The following selects people that are in school or employed.
people.filter( people("inSchool") || people("isEmployed") )

def when(condition: Column, value: Any): Column

Evaluates a list of conditions and returns one of multiple possible result expressions. If otherwise is not defined at the end, null is returned for unmatched conditions.

// Example: encoding gender string column into integer.

// Scala:
people.select(when(people("gender") === "male", 0)
  .when(people("gender") === "female", 1)
  .otherwise(2))


**** Mathematical Functions on the Columns 

import org.apache.spark.sql.functions._

df.select( df("col1"), sqrt(df("col1")) )
|col1|        SQRT(col1)|
+----+------------------+
|   1|               1.0|
|   2|1.4142135623730951|


**** Displaying data and Schema 

df.show
df.printSchema

**** SQL OPERATIONS 

** SELECT WHERE 
		df.select("col1","col2").where($"col1" === 1)

** GROUP BY AGGREGATE 

		df.groupBy("col1").agg(sum("col2").as("sum_total"))

Note that the groupBy() method returns a GroupedData object,

val df = Seq(("Yoda",             "Obi-Wan Kenobi"),
             ("Anakin Skywalker", "Sheev Palpatine"),
             ("Luke Skywalker",   "Han Solo, Leia Skywalker"),
             ("Leia Skywalker",   "Obi-Wan Kenobi"),
             ("Sheev Palpatine",  "Anakin Skywalker"),
             ("Han Solo",         "Leia Skywalker, Luke Skywalker, Obi-Wan Kenobi, Chewbacca"),
             ("Obi-Wan Kenobi",   "Yoda, Qui-Gon Jinn"),
             ("R2-D2",            "C-3PO"),
             ("C-3PO",            "R2-D2"),
             ("Darth Maul",       "Sheev Palpatine"),
             ("Chewbacca",        "Han Solo"),
             ("Lando Calrissian", "Han Solo"),
             ("Jabba",            "Boba Fett")
            )
.toDF("name", "friends")

friends_ds.show()
+----------------+--------------------+
|            name|             friends|
+----------------+--------------------+
|            Yoda|      Obi-Wan Kenobi|
|Anakin Skywalker|     Sheev Palpatine|
|  Luke Skywalker|Han Solo, Leia Sk...|
|  Leia Skywalker|      Obi-Wan Kenobi|
| Sheev Palpatine|    Anakin Skywalker|
|        Han Solo|Leia Skywalker, L...|
|  Obi-Wan Kenobi|  Yoda, Qui-Gon Jinn|
|           R2-D2|               C-3PO|
|           C-3PO|               R2-D2|
|      Darth Maul|     Sheev Palpatine|
|       Chewbacca|            Han Solo|
|Lando Calrissian|            Han Solo|
|           Jabba|           Boba Fett|
+----------------+--------------------+

======================================================================================================================================================================================================

Reading Json File 

{"name":"Michael", "schools":[{"sname":"stanford", "year":2010}, {"sname":"berkeley", "year":2012}]}
{"name":"Andy", "schools":[{"sname":"ucsb", "year":2011}]}


val people = sqlContext.read.json("people.json")
people: org.apache.spark.sql.DataFrame

>> people.show()x
+-------+--------------------+
|   name|             schools|
+-------+--------------------+
|Michael|[[stanford,2010],...|
|   Andy|       [[ucsb,2011]]|
+-------+--------------------+

If your JSON object contains nested arrays of structs, how will you access the elements of an array? One way is by flattening it.  We have to use the Explode() Function 

import org.apache.spark.sql.functions._
val flattened = people.select($"name",explode($"schools").as("schools_flat"))

The struct has two fields: "sname" and "year". We will select only the school name, "sname":
val schools = flattened.select("name","schools_flat.sname","schools_flat.schools")












============================================================================================================================================================================================================
Apache Spark Prod cluster size
Vaquar Khan edited this page on May 23 · 3 revisions

A 5 data nodes - Each node of 250 GB ram

C 32 GB and cluster size 64 GB

B Executor memory 8gb, driver memory 5gb, executor core 1 and num executors 5

hortonworks:

Master-1

Memory per node :524GB Disk per node : 4*1TB HDD Space after RAID :2.725TB Space Allocated for Execution : 1TB

Slaves-5 Memory per node :128 GB Disk per node : 4*1TB HDD Space after RAID :2.725TB Space Allocated for Execution : 1TB


=========================================================================================================================================================================================

 
 
 SOURCE: APACHE   |   MAR 16, 2017
 COLLECT  
 val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

val df = sqlContext.read.json("examples/src/main/resources/people.json")

// Displays the content of the DataFrame to stdout
df.show() val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// Create the DataFrame
val df = sqlContext.read.json("examples/src/main/resources/people.json")

// Show the content of the DataFrame
df.show()
// age  name
// null Michael
// 30   Andy
// 19   Justin

// Print the schema in a tree format
df.printSchema()
// root
// |-- age: long (nullable = true)
// |-- name: string (nullable = true)

// Select only the "name" column
df.select("name").show()
// name
// Michael
// Andy
// Justin

// Select everybody, but increment the age by 1
df.select(df("name"), df("age") + 1).show()
// name    (age + 1)
// Michael null
// Andy    31
// Justin  20

// Select people older than 21
df.filter(df("age") > 21).show()
// age name
// 30  Andy

// Count people by age
df.groupBy("age").count().show()
// age  count
// null 1
// 19   1
// 30   1 val sqlContext = ...  // An existing SQLContext
val df = sqlContext.sql("SELECT * FROM table")
The Scala interface for Spark SQL supports automatically converting an RDD containing case classes to a DataFrame. The case class defines the schema of the table. The names of the arguments to the case class are read using reflection and become the names of the columns. Case classes can also be nested or contain complex types such as Sequences or Arrays. This RDD can be implicitly converted to a DataFrame and then be registered as a table. Tables can be used in subsequent SQL statements.

// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
// this is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._

// Define the schema using a case class.
// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
// you can use custom classes that implement the Product interface.
case class Person(name: String, age: Int)

// Create an RDD of Person objects and register it as a table.
val people = sc.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(p => Person(p(0), p(1).trim.toInt)).toDF()
people.registerTempTable("people")

// SQL statements can be run by using the sql methods provided by sqlContext.
val teenagers = sqlContext.sql("SELECT name, age FROM people WHERE age >= 13 AND age <= 19")

// The results of SQL queries are DataFrames and support all the normal RDD operations.
// The columns of a row in the result can be accessed by field index:
teenagers.map(t => "Name: " + t(0)).collect().foreach(println)

// or by field name:
teenagers.map(t => "Name: " + t.getAs[String]("name")).collect().foreach(println)

// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]
teenagers.map(_.getValuesMap[Any](List("name", "age"))).collect().foreach(println)
// Map("name" -> "Justin", "age" -> 19)
When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps.

Create an RDD of Rows from the original RDD;
Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.
Apply the schema to the RDD of Rows via createDataFrame method provided by SQLContext.
For example:

// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// Create an RDD
val people = sc.textFile("examples/src/main/resources/people.txt")

// The schema is encoded in a string
val schemaString = "name age"

// Import Row.
import org.apache.spark.sql.Row;

// Import Spark SQL data types
import org.apache.spark.sql.types.{StructType,StructField,StringType};

// Generate the schema based on the string of schema
val schema =
  StructType(
    schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, true)))

// Convert records of the RDD (people) to Rows.
val rowRDD = people.map(_.split(",")).map(p => Row(p(0), p(1).trim))

// Apply the schema to the RDD.
val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema)

// Register the DataFrames as a table.
peopleDataFrame.registerTempTable("people")

// SQL statements can be run by using the sql methods provided by sqlContext.
val results = sqlContext.sql("SELECT name FROM people")

// The results of SQL queries are DataFrames and support all the normal RDD operations.
// The columns of a row in the result can be accessed by field index or by field name.
results.map(t => "Name: " + t(0)).collect().foreach(println) val df = sqlContext.read.load("examples/src/main/resources/users.parquet")
df.select("name", "favorite_color").write.save("namesAndFavColors.parquet") val df = sqlContext.read.format("json").load("examples/src/main/resources/people.json")
df.select("name", "age").write.format("parquet").save("namesAndAges.parquet")
Scala/Java	Python	Meaning
SaveMode.ErrorIfExists (default)	"error" (default)	When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.
SaveMode.Append	"append"	When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.
SaveMode.Overwrite	"overwrite"	Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.
SaveMode.Ignore	"ignore"	Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.
// sqlContext from the previous example is used in this example.
// This is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._

val people: RDD[Person] = ... // An RDD of case class objects, from the previous example.

// The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet.
people.write.parquet("people.parquet")

// Read in the parquet file created above.  Parquet files are self-describing so the schema is preserved.
// The result of loading a Parquet file is also a DataFrame.
val parquetFile = sqlContext.read.parquet("people.parquet")

//Parquet files can also be registered as tables and then used in SQL statements.
parquetFile.registerTempTable("parquetFile")
val teenagers = sqlContext.sql("SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19")
teenagers.map(t => "Name: " + t(0)).collect().foreach(println) path
└── to
    └── table
        ├── gender=male
        │   ├── ...
        │   │
        │   ├── country=US
        │   │   └── data.parquet
        │   ├── country=CN
        │   │   └── data.parquet
        │   └── ...
        └── gender=female
            ├── ...
            │
            ├── country=US
            │   └── data.parquet
            ├── country=CN
            │   └── data.parquet
            └── ... root
|-- name: string (nullable = true)
|-- age: long (nullable = true)
|-- gender: string (nullable = true)
|-- country: string (nullable = true) // sqlContext from the previous example is used in this example.
// This is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._

// Create a simple DataFrame, stored into a partition directory
val df1 = sc.makeRDD(1 to 5).map(i => (i, i * 2)).toDF("single", "double")
df1.write.parquet("data/test_table/key=1")

// Create another DataFrame in a new partition directory,
// adding a new column and dropping an existing column
val df2 = sc.makeRDD(6 to 10).map(i => (i, i * 3)).toDF("single", "triple")
df2.write.parquet("data/test_table/key=2")

// Read the partitioned table
val df3 = sqlContext.read.parquet("data/test_table")
df3.printSchema()

// The final schema consists of all 3 columns in the Parquet files together
// with the partitioning column appeared in the partition directory paths.
// root
// |-- single: int (nullable = true)
// |-- double: int (nullable = true)
// |-- triple: int (nullable = true)
// |-- key : int (nullable = true)
Property Name	Default	Meaning
spark.sql.parquet.binaryAsString	false	Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.
spark.sql.parquet.int96AsTimestamp	true	Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.
spark.sql.parquet.cacheMetadata	true	Turns on caching of Parquet schema metadata. Can speed up querying of static data.
spark.sql.parquet.compression.codec	gzip	Sets the compression codec use when writing Parquet files. Acceptable values include: uncompressed, snappy, gzip, lzo.
spark.sql.parquet.filterPushdown	false	Turn on Parquet filter pushdown optimization. This feature is turned off by default because of a known bug in Parquet 1.6.0rc3 (PARQUET-136). However, if your table doesn't contain any nullable string or binary columns, it's still safe to turn this feature on.
spark.sql.hive.convertMetastoreParquet	true	When set to false, Spark SQL will use the Hive SerDe for parquet tables instead of the built in support.
Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. This conversion can be done using SQLContext.read.json() on either an RDD of String, or a JSON file.

Note that the file that is offered as a json file is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail.

// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// A JSON dataset is pointed to by path.
// The path can be either a single text file or a directory storing text files.
val path = "examples/src/main/resources/people.json"
val people = sqlContext.read.json(path)

// The inferred schema can be visualized using the printSchema() method.
people.printSchema()
// root
//  |-- age: integer (nullable = true)
//  |-- name: string (nullable = true)

// Register this DataFrame as a table.
people.registerTempTable("people")

// SQL statements can be run by using the sql methods provided by sqlContext.
val teenagers = sqlContext.sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")

// Alternatively, a DataFrame can be created for a JSON dataset represented by
// an RDD[String] storing one JSON object per string.
val anotherPeopleRDD = sc.parallelize(
  """{"name":"Yin","address":{"city":"Columbus","state":"Ohio"}}""" :: Nil)
val anotherPeople = sqlContext.read.json(anotherPeopleRDD)
When working with Hive one must construct a HiveContext, which inherits from SQLContext, and adds support for finding tables in the MetaStore and writing queries using HiveQL. Users who do not have an existing Hive deployment can still create a HiveContext. When not configured by the hive-site.xml, the context automatically creates metastore_db and warehouse in the current directory.

// sc is an existing SparkContext.
val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)

sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
sqlContext.sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")

// Queries are expressed in HiveQL
sqlContext.sql("FROM src SELECT key, value").collect().foreach(println)
Property Name	Default	Meaning
spark.sql.hive.metastore.version	0.13.1	Version of the Hive metastore. Available options are 0.12.0 and 0.13.1. Support for more versions is coming in the future.
spark.sql.hive.metastore.jars	builtin	Location of the jars that should be used to instantiate the HiveMetastoreClient. This property can be one of three options:
builtin
Use Hive 0.13.1, which is bundled with the Spark assembly jar when -Phive is enabled. When this option is chosen, spark.sql.hive.metastore.version must be either 0.13.1 or not defined.
maven
Use Hive jars of specified version downloaded from Maven repositories.
A classpath in the standard format for both Hive and Hadoop.
spark.sql.hive.metastore.sharedPrefixes	com.mysql.jdbc,
org.postgresql,
com.microsoft.sqlserver,
oracle.jdbc	
A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.

spark.sql.hive.metastore.barrierPrefixes	(empty)	
A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. org.apache.spark.*).

SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell
Property Name	Meaning
url	The JDBC URL to connect to.
dbtable	The JDBC table that should be read. Note that anything that is valid in a FROM clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses.
driver	The class name of the JDBC driver needed to connect to this URL. This class will be loaded on the master and workers before running an JDBC commands to allow the driver to register itself with the JDBC subsystem.
partitionColumn, lowerBound, upperBound, numPartitions	These options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric column from the table in question. Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned.
val jdbcDF = sqlContext.load("jdbc", Map(
  "url" -> "jdbc:postgresql:dbserver",
  "dbtable" -> "schema.tablename"))
Property Name	Default	Meaning
spark.sql.inMemoryColumnarStorage.compressed	true	When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.
spark.sql.inMemoryColumnarStorage.batchSize	10000	Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.
Property Name	Default	Meaning
spark.sql.autoBroadcastJoinThreshold	10485760 (10 MB)	Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command ANALYZE TABLE <tableName> COMPUTE STATISTICS noscan has been run.
spark.sql.codegen	false	When true, code will be dynamically generated at runtime for expression evaluation in a specific query. For some queries with complicated expression this option can lead to significant speed-ups. However, for simple queries this can actually slow down query execution.
spark.sql.shuffle.partitions	200	Configures the number of partitions to use when shuffling data for joins or aggregations.
spark.sql.planner.externalSort	false	When true, performs sorts spilling to disk as needed otherwise sort each partition in memory.
export HIVE_SERVER2_THRIFT_PORT=<listening-port>
export HIVE_SERVER2_THRIFT_BIND_HOST=<listening-host>
./sbin/start-thriftserver.sh \
  --master <master-uri> \
  ... ./sbin/start-thriftserver.sh \
  --hiveconf hive.server2.thrift.port=<listening-port> \
  --hiveconf hive.server2.thrift.bind.host=<listening-host> \
  --master <master-uri>
  ... // In 1.3.x, in order for the grouping column "department" to show up,
// it must be included explicitly as part of the agg function call.
df.groupBy("department").agg($"department", max("age"), sum("expense"))

// In 1.4+, grouping column "department" is included automatically.
df.groupBy("department").agg(max("age"), sum("expense"))

// Revert to 1.3 behavior (not retaining grouping column) by:
sqlContext.setConf("spark.sql.retainGroupColumns", "false") sqlContext.udf.register("strLen", (s: String) => s.length())
All data types of Spark SQL are located in the package org.apache.spark.sql.types. You can access them by doing

import  org.apache.spark.sql.types._
Data type	Value type in Scala	API to access or create a data type
ByteType	Byte	ByteType
ShortType	Short	ShortType
IntegerType	Int	IntegerType
LongType	Long	LongType
FloatType	Float	FloatType
DoubleType	Double	DoubleType
DecimalType	java.math.BigDecimal	DecimalType
StringType	String	StringType
BinaryType	Array[Byte]	BinaryType
BooleanType	Boolean	BooleanType
TimestampType	java.sql.Timestamp	TimestampType
DateType	java.sql.Date	DateType
ArrayType	scala.collection.Seq	ArrayType(elementType, [containsNull])
Note: The default value of containsNull is true.
MapType	scala.collection.Map	MapType(keyType, valueType, [valueContainsNull])
Note: The default value of valueContainsNull is true.
StructType	org.apache.spark.sql.Row	StructType(fields)
Note: fields is a Seq of StructFields. Also, two fields with the same name are not allowed.
StructField	The value type in Scala of the data type of this field (For example, Int for a StructField with the data type IntegerType)	StructField(name, dataType, nullable)

