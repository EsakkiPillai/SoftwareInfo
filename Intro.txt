val src = sc.textFile()
val lines = src.flatMap( line => line.split(" "))
val word = lines.map( x => (x,1))
val counts = word.reducebykey( x,y =>  x+y)
counts.saveAstextFile("path to the File")

we can also use the below Method as Well 
val src = sc.textFile()
val lines = src.flatMap( line => line.split(" "))
val counts = lines.countByValues()  """ it will return the Map Collection """


First Sample Application 


object WordCount{

def main (args:Array[String]){

// Create A Spark Session 

val conf = new sparkConf().setAppName("First Demo Appln")
val sc = new sparkContext(conf)

the Above Two Lines will be Equivalent to opening a Spark-scala Terminal => which already have sc value  the above configuration is for spark 1.6 for 2.0 we have to call the SparkSession 

val src = sc.textFile()
val lines = src.flatMap( line => line.split(" "))
val word = lines.map( x => (x,1)) 
val counts = word.reducebykey( x,y =>  x+y)             word is of RDD rdd dont have reducebyKey Method so scala will convert this rdd in to the form which has the method in our case rdd will be transfered in to PAIR RDDFunctions 
counts.saveAstextFile("path to the File")


}
}


Tro sumbit Appln 

spark-submit --class "jarname" "InputFilePath" "OutputFilePath"
===================================================================================================================================================================================================================

HOW AGGREGATES WORKS :- 

First of all Thanks to Diego's reply which helped me connect the dots in understanding aggregate() function..

Let me confess that I couldn't sleep last night properly because I couldn't get how aggregate() works internally, I'll get good sleep tonight definitely :-)

Let's start understanding it

val result = List(1,2,3,4,5,6,7,8,9,10).par.aggregate((0, 0))
         (
          (x, y) => (x._1 + y, x._2 + 1), 
          (x,y) =>(x._1 + y._1, x._2 + y._2)
         )
result: (Int, Int) = (55,10)

aggregate function has 3 parts :

initial value of accumulators : tuple(0,0) here
seqop : It works like foldLeft with initial value of 0
combop : It combines the result generated through parallelization (this part was difficult for me to understand)
Let's understand all 3 parts independently :

part-1 : Initial tuple (0,0)

Aggregate() starts with initial value of accumulators x which is (0,0) here. First tuple x._1 which is initially 0 is used to compute the sum, Second tuple x._2 is used to compute total number of elements in the list.

part-2 : (x, y) => (x._1 + y, x._2 + 1)

If you know how foldLeft works in scala then it should be easy to understand this part. Above function works just like foldLeft on our List(1,2,3,4...10).

Iteration#      (x._1 + y, x._2 + 1)
     1           (0+1, 0+1)
     2           (1+2, 1+1)
     3           (3+3, 2+1)
     4           (6+4, 3+1)
     .             ....
     .             ....
     10          (45+10, 9+1)
thus after all 10 iteration you'll get the result (55,10). If you understand this part the rest is very easy but for me it was the most difficult part in understanding if all the required computation are finished then what is the use of second part i.e. compop - stay tuned :-)

part 3 : (x,y) =>(x._1 + y._1, x._2 + y._2)

Well this 3rd part is combOp which combines the result generated by different threads during parallelization, remember we used 'par' in our code to enable parallel computation of list :

List(1,2,3,4,5,6,7,8,9,10).par.aggregate(....)

Apache spark is effectively using aggregate function to do parallel computation of RDD.

Let's assume that our List(1,2,3,4,5,6,7,8,9,10) is being computed by 3 threads in parallel. Here each thread is working on partial list and then our aggregate() combOp will combine the result of each thread's computation using the below code :

(x,y) =>(x._1 + y._1, x._2 + y._2)
Original list : List(1,2,3,4,5,6,7,8,9,10)

Thread1 start computing on partial list say (1,2,3,4), Thread2 computes (5,6,7,8) and Thread3 computes partial list say (9,10)

At the end of computation, Thread-1 result will be (10,4), Thread-2 result will be (26,4) and Thread-3 result will be (19,2).

At the end of parallel computation, we'll have ((10,4),(26,4),(19,2))

Iteration#      (x._1 + y, x._2 + 1)
     1           (0+10, 0+4)
     2           (10+26, 4+4)
     3           (36+19, 8+2)
which is (55,10).

Finally let me re-iterate that seqOp job is to compute the sum of all the elements of list and total number of list whereas combine function's job is to combine different partial result generated during parallelization.

I hope above explanation help you understand the aggregate().



def computeAvg(input: RDD[Int]) = {
    input.aggregate((0, 0))((x, y) => (x._1 + y, x._2 + 1),
      (x,y) => (x._1 + y._1, x._2 + y._2))
  }
  
  
  val avgFoldLeft  = (src.foldLeft(0)((r,c)=> r+c )) / (src.foldLeft(0)((sum,_) => sum+1))
====================================================================================================================================================================================================================================




val master = args.length match {
case x :Int if x> 0 => arg[0] 
case _ => "local"
}

if the length of the Argument Matches is Greater than 0 then it will take the atgs[0] if not it will take the argument as "Local"


===========================================================================================================================================================================================






def parsedata(line:String)  = {


val rdd = src.map(line => line.split(","))
val custid= rdd.map( x => x_.1)
val amount = rdd.map( x => x._2)
(custid ,amount)

}
val src = sc.textFile()
val data = src.map(parsedata)
val  group = data.mapValues( x => (x ,1) )    (44 ,( 
val count = group.reducebyKey((x,y) => (x._1+y._1 , x._2+y._2))
val result = count.map ( x =>  val custid = x._1 val amountSpend = x._2.tofloat()  ( amountspend , custid).sortbyKey())
result.collect()




-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

ReduceByKey 

it will take 2 values (x,y) => dataset (k1,v1) (k2,v2) (k1,v2) (k2,v10)

src.reduceByKey((x,y) => (x+y) )

if the dataaset is like (k1,(v1,v2)) (k2,(v2,v4))

src.reduceByKey((x,y) => (x._1 + y._1 , x._2 + y._2 )

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
We Should Avoid GroupByKey whenever is Possible , it will shufflle and aggregate it so it will involve in huge network performance in the Cluster
groupByKey()

groupBykey always  groupbased on the Key 


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Fold 

it requires a initial Value 


Step 1:- 
Update build.sbt with below log4j dependency
libraryDependencies += "log4j" % "log4j" % "1.2.14"

step 2:- 
create Log4j.properties file under src/main/resources and update as follows:

# Define the root logger with appender file
log = /tmp/log4j
log4j.rootLogger = DEBUG, FILE

# Define the file appender
log4j.appender.FILE=org.apache.log4j.FileAppender
log4j.appender.FILE.File=${log}/log.out

# Define the layout for file appender
log4j.appender.FILE.layout=org.apache.logs4j.PatternLayout
log4j.appender.FILE.layout.conversionPattern=%m%n
SBT run, can be seen in /tmp/log4j/log.out log

Step 3:- Can be used in code as follows:

import org.apache.log4j.Logger

object HelloWorld {
  val logger = Logger.getLogger(this.getClass.getName)
  def main(args: Array[String]): Unit = {
    logger.info("Logger : Welcome to log4j")
  }
}


================================================================================================================================================================================================
Save Modes
Save operations can optionally take a SaveMode, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing a Overwrite, the data will be deleted before writing out the new data.

Scala/Java							Any Language			Meaning
SaveMode.ErrorIfExists (default)	"error" (default)	When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.
SaveMode.Append						"append"			When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.
SaveMode.Overwrite					"overwrite"			Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.
SaveMode.Ignore						"ignore"			Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.




df.select("name", "age", "sex").write.mode(SaveMode.Append).format("parquet").save("/user/esakkipillai/person")

df.write.parquet("/user/esakkipillai/person_2")
Second statement fails to execute since it takes save mode as an "error" and raises below error.
org.apache.spark.sql.AnalysisException : path hdfs://cloudera1:8020//user/esakkipillai/person already exists


**When Working with the HIveContext, we can store the dataframe as persistent table using saveAsTable Command.
**Unlike the registerTempTable command it will be materialize the content of the dataframe and create a pointer to the data in the hive metastore.
**Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore
**By default saveAsTable will create a “managed table”
** data will be available location will be controlled by the metastore 
** When the Managed table is dropped data will be dropped as well 

**sql supports both reading and writiing parquet files 
** parquet file is a columnar format it automatically preserves the schema of the data 
**when writing the data to the parquet files, all the columns are automatically converted to null for compatibility reasons 


NaN Semantics
There is specially handling for not-a-number (NaN) when dealing with float or double types that does not exactly match standard floating point semantics. Specifically:

NaN = NaN returns true.
In aggregations all NaN values are grouped together.
NaN is treated as a normal value in join keys.
NaN values go last when in ascending order, larger than any other numeric value.


**** DataFrames 
**In Spark, a DataFrame is a distributed collection of data organized into named columns. 
**It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. 
**DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.

**** DataFrames are evaluted lazily 
**** which means no computation take place until an action is  performed 
**** action is any method that not produces dataframe, Such as data on the console converting data into scala arrays storing in a file 

rdd.toDF()  			// this implicit conversion creates a DataFrame with column name _1 and _2
rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"

Column Level Operations 

def /(other: Any): Column   Division this expression by another expression. 
// Scala: The following divides a person's height by their weight.
			people.select( people("height") / people("weight") )


def ===(other: Any): Column 	Equality test.	
// Scala:  checking the columns 
df.filter( df("colA") === df("colB") )


// Scala: The following selects people older than 21.
people.select( people("age") > 21 )
// Renames colA to colB in select output.
df.select($"colA".alias("colB"))
// Renames colA to colB in select output.
df.select($"colA".as('colB))

// Scala: sort a DataFrame by age column in ascending order.
df.sort(df("age").asc)

// Casts colA to IntegerType.
import org.apache.spark.sql.types.IntegerType
df.select(df("colA").cast(IntegerType))

// Scala
df.sort(df("age").desc)

def isNaN: Column
True if the current expression is NaN.

def isNotNull: Column
True if the current expression is NOT null.

def isNull: Column
True if the current expression is  null.

def substr(startPos: Int, len: Int): Column
An expression that returns a substring.

// Scala: The following selects people that are in school or employed.
people.filter( people("inSchool") || people("isEmployed") )

def when(condition: Column, value: Any): Column

Evaluates a list of conditions and returns one of multiple possible result expressions. If otherwise is not defined at the end, null is returned for unmatched conditions.

// Example: encoding gender string column into integer.

// Scala:
people.select(when(people("gender") === "male", 0)
  .when(people("gender") === "female", 1)
  .otherwise(2))


**** Mathematical Functions on the Columns 

import org.apache.spark.sql.functions._

df.select( df("col1"), sqrt(df("col1")) )
|col1|        SQRT(col1)|
+----+------------------+
|   1|               1.0|
|   2|1.4142135623730951|


**** Displaying data and Schema 

df.show
df.printSchema

**** SQL OPERATIONS 

** SELECT WHERE 
		df.select("col1","col2").where($"col1" === 1)

** GROUP BY AGGREGATE 

		df.groupBy("col1").agg(sum("col2").as("sum_total"))

Note that the groupBy() method returns a GroupedData object,

val df = Seq(("Yoda",             "Obi-Wan Kenobi"),
             ("Anakin Skywalker", "Sheev Palpatine"),
             ("Luke Skywalker",   "Han Solo, Leia Skywalker"),
             ("Leia Skywalker",   "Obi-Wan Kenobi"),
             ("Sheev Palpatine",  "Anakin Skywalker"),
             ("Han Solo",         "Leia Skywalker, Luke Skywalker, Obi-Wan Kenobi, Chewbacca"),
             ("Obi-Wan Kenobi",   "Yoda, Qui-Gon Jinn"),
             ("R2-D2",            "C-3PO"),
             ("C-3PO",            "R2-D2"),
             ("Darth Maul",       "Sheev Palpatine"),
             ("Chewbacca",        "Han Solo"),
             ("Lando Calrissian", "Han Solo"),
             ("Jabba",            "Boba Fett")
            )
.toDF("name", "friends")

friends_ds.show()
+----------------+--------------------+
|            name|             friends|
+----------------+--------------------+
|            Yoda|      Obi-Wan Kenobi|
|Anakin Skywalker|     Sheev Palpatine|
|  Luke Skywalker|Han Solo, Leia Sk...|
|  Leia Skywalker|      Obi-Wan Kenobi|
| Sheev Palpatine|    Anakin Skywalker|
|        Han Solo|Leia Skywalker, L...|
|  Obi-Wan Kenobi|  Yoda, Qui-Gon Jinn|
|           R2-D2|               C-3PO|
|           C-3PO|               R2-D2|
|      Darth Maul|     Sheev Palpatine|
|       Chewbacca|            Han Solo|
|Lando Calrissian|            Han Solo|
|           Jabba|           Boba Fett|
+----------------+--------------------+

======================================================================================================================================================================================================

Reading Json File 

{"name":"Michael", "schools":[{"sname":"stanford", "year":2010}, {"sname":"berkeley", "year":2012}]}
{"name":"Andy", "schools":[{"sname":"ucsb", "year":2011}]}


val people = sqlContext.read.json("people.json")
people: org.apache.spark.sql.DataFrame

>> people.show()
+-------+--------------------+
|   name|             schools|
+-------+--------------------+
|Michael|[[stanford,2010],...|
|   Andy|       [[ucsb,2011]]|
+-------+--------------------+

If your JSON object contains nested arrays of structs, how will you access the elements of an array? One way is by flattening it.  We have to use the Explode() Function 

import org.apache.spark.sql.functions._
val flattened = people.select($"name",explode($"schools").as("schools_flat"))

The struct has two fields: "sname" and "year". We will select only the school name, "sname":
val schools = flattened.select("name","schools_flat.sname","schools_flat.schools")












============================================================================================================================================================================================================
Apache Spark Prod cluster size
Vaquar Khan edited this page on May 23 · 3 revisions

A 5 data nodes - Each node of 250 GB ram

C 32 GB and cluster size 64 GB

B Executor memory 8gb, driver memory 5gb, executor core 1 and num executors 5

hortonworks:

Master-1

Memory per node :524GB Disk per node : 4*1TB HDD Space after RAID :2.725TB Space Allocated for Execution : 1TB

Slaves-5 Memory per node :128 GB Disk per node : 4*1TB HDD Space after RAID :2.725TB Space Allocated for Execution : 1TB



